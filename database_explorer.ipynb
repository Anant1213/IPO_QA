{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPO Intelligence Database Explorer üóÑÔ∏è\n",
    "\n",
    "This notebook connects directly to the PostgreSQL database and allows you to explore the data.\n",
    "\n",
    "## Database Schema\n",
    "- **documents** - Document metadata\n",
    "- **chapters** - Chapter information\n",
    "- **chunks** - Text chunks from documents\n",
    "- **embeddings** - Vector embeddings (384-dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mJupyter cannot be started. Error attempting to locate Jupyter: Running cells with 'Python 3.12.3' requires the notebook package.\n",
      "\u001b[1;31mInstall 'jupyter and notebook' into the Python environment. \n",
      "\u001b[1;31mCommand: 'python -m pip install jupyter notebook -U\n",
      "\u001b[1;31mor\n",
      "\u001b[1;31mconda install jupyter notebook -U'\n",
      "\u001b[1;31mClick <a href='https://aka.ms/installJupyterForVSCode'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "# Setup - Run this first\n",
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from database.connection import get_db, test_connection\n",
    "from database.repositories import DocumentRepository, ChunkRepository, EmbeddingRepository\n",
    "from sqlalchemy import text\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "test_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. View All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all documents\n",
    "docs = DocumentRepository.get_all()\n",
    "\n",
    "# Convert to DataFrame for better display\n",
    "df_docs = pd.DataFrame(docs)\n",
    "df_docs[['document_id', 'display_name', 'total_pages', 'total_chunks', 'upload_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query Documents Table Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw SQL query\n",
    "with get_db() as db:\n",
    "    query = text(\"\"\"\n",
    "        SELECT \n",
    "            document_id,\n",
    "            display_name,\n",
    "            total_pages,\n",
    "            total_chunks,\n",
    "            file_hash,\n",
    "            upload_date\n",
    "        FROM documents\n",
    "        ORDER BY created_at DESC\n",
    "    \"\"\")\n",
    "    result = db.execute(query)\n",
    "    df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Chunks for a Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to explore different documents\n",
    "DOCUMENT_ID = 'pw_ipo'  # or 'policybazar_ipo', 'emt_ipo', 'lenskart_solutions_limited_drhp_1753782641'\n",
    "\n",
    "# Get chunks\n",
    "chunks = ChunkRepository.get_by_document(DOCUMENT_ID)\n",
    "\n",
    "print(f\"üì¶ Found {len(chunks)} chunks for {DOCUMENT_ID}\\n\")\n",
    "print(\"First 5 chunks:\")\n",
    "pd.DataFrame(chunks[:5])[['chunk_index', 'page_number', 'word_count', 'text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Database Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db() as db:\n",
    "    stats_query = text(\"\"\"\n",
    "        SELECT \n",
    "            d.document_id,\n",
    "            d.display_name,\n",
    "            COUNT(DISTINCT c.id) as chunk_count,\n",
    "            COUNT(DISTINCT e.id) as embedding_count,\n",
    "            AVG(c.word_count) as avg_chunk_words\n",
    "        FROM documents d\n",
    "        LEFT JOIN chunks c ON c.document_id = d.id\n",
    "        LEFT JOIN embeddings e ON e.chunk_id = c.id\n",
    "        GROUP BY d.id\n",
    "        ORDER BY d.created_at DESC\n",
    "    \"\"\")\n",
    "    result = db.execute(stats_query)\n",
    "    stats_df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Vector Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random query vector\n",
    "query_vector = np.random.rand(384).tolist()\n",
    "\n",
    "# Search for similar chunks\n",
    "results = EmbeddingRepository.search_similar(\n",
    "    query_embedding=query_vector,\n",
    "    document_id='pw_ipo',\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(f\"üîç Found {len(results)} similar chunks:\\n\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    print(f\"{i}. Similarity: {r['similarity']:.4f}\")\n",
    "    print(f\"   Page: {r['page_number']}\")\n",
    "    print(f\"   Text: {r['text'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Search Chunks by Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for chunks containing specific text\n",
    "search_term = 'revenue'  # Change this to search for different terms\n",
    "\n",
    "with get_db() as db:\n",
    "    search_query = text(\"\"\"\n",
    "        SELECT \n",
    "            d.display_name,\n",
    "            c.chunk_index,\n",
    "            c.page_number,\n",
    "            c.text\n",
    "        FROM chunks c\n",
    "        JOIN documents d ON d.id = c.document_id\n",
    "        WHERE LOWER(c.text) LIKE :search_term\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    result = db.execute(search_query, {'search_term': f'%{search_term.lower()}%'})\n",
    "    search_df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "print(f\"Found {len(search_df)} chunks containing '{search_term}':\\n\")\n",
    "search_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom SQL Queries\n",
    "\n",
    "Write your own SQL queries here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find longest chunks\n",
    "with get_db() as db:\n",
    "    custom_query = text(\"\"\"\n",
    "        SELECT \n",
    "            d.display_name,\n",
    "            c.chunk_index,\n",
    "            c.page_number,\n",
    "            c.word_count,\n",
    "            LEFT(c.text, 100) as text_preview\n",
    "        FROM chunks c\n",
    "        JOIN documents d ON d.id = c.document_id\n",
    "        ORDER BY c.word_count DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    result = db.execute(custom_query)\n",
    "    df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Database Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_db() as db:\n",
    "    # Get table sizes\n",
    "    health_query = text(\"\"\"\n",
    "        SELECT \n",
    "            'documents' as table_name,\n",
    "            COUNT(*) as row_count\n",
    "        FROM documents\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'chapters' as table_name,\n",
    "            COUNT(*) as row_count\n",
    "        FROM chapters\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'chunks' as table_name,\n",
    "            COUNT(*) as row_count\n",
    "        FROM chunks\n",
    "        \n",
    "        UNION ALL\n",
    "        \n",
    "        SELECT \n",
    "            'embeddings' as table_name,\n",
    "            COUNT(*) as row_count\n",
    "        FROM embeddings\n",
    "    \"\"\")\n",
    "    result = db.execute(health_query)\n",
    "    health_df = pd.DataFrame(result.fetchall(), columns=result.keys())\n",
    "\n",
    "print(\"üìä Database Health Status:\\n\")\n",
    "health_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Data\n",
    "\n",
    "Export query results to CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export documents to CSV\n",
    "df_docs.to_csv('documents_export.csv', index=False)\n",
    "print(\"‚úÖ Exported to documents_export.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
