# IPO Intelligence Platform ğŸš€

A robust **Hybrid RAG (Retrieval-Augmented Generation)** system analyzing IPO prospectus documents using both **Knowledge Graphs** and **Vector Embeddings**.

Built with Flask, NetworkX, SentenceTransformers, and Ollama (Llama 3).

![IPO Intelligence Platform](https://placeholder-image-url-or-relative-path.png)

## ğŸŒŸ Key Features

*   **Hybrid RAG Architecture**: Intelligently combines:
    *   **Knowledge Graph RAG**: For structured queries (e.g., "Who is the CEO?", "What are the subsidiaries?") using precise entity-relationship traversal.
    *   **Vector RAG**: For semantic queries (e.g., "What are the risk factors?") using embedding-based search.
    *   **Context Fusion**: Merges both sources for complex reasoning.
*   **Query Router**: Automatically classifies user questions to route them to the most efficient retrieval engine.
*   **Interactive Knowledge Graph**: Visualize relationships between entities in real-time.
*   **Local LLM Support**: Fully private execution using local Ollama models (Llama 3, DeepSeek).
*   **Streaming Responses**: Real-time token streaming for a responsive UI.

## ğŸ—ï¸ System Architecture

The system uses a **Router-to-Solver** pattern to optimize for both accuracy and latency.

```mermaid
graph TD
    User[User Question] --> Router{Query Router}
    Router -->|Structure| KG[Knowledge Graph RAG]
    Router -->|Semantic| Vector[Vector RAG]
    Router -->|Complex| Hybrid[Hybrid RAG]
    KG --> Final[Context Fusion]
    Vector --> Final
    Hybrid --> Final
    Final --> LLM[Ollama Llama3]
```

See [Detailed Architecture](docs/architecture_diagram.md) for deeper insights.

## ğŸš€ Getting Started

### Prerequisites

*   Python 3.10+
*   [Ollama](https://ollama.ai) installed and running
*   `llama3` model pulled (`ollama pull llama3`)

### Installation

1.  **Clone the repository**
    ```bash
    git clone https://github.com/yourusername/ipo-qa.git
    cd ipo-qa
    ```

2.  **Create a Virtual Environment**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

    > **Note for Apple Silicon (M1/M2)**: If you encounter mutex lock errors with Torch, ensure you are running in a clean environment and let the requirements install the correct wheels.

### Running the Application

1.  **Start Ollama Server** (in a separate terminal)
    ```bash
    ollama serve
    ```

2.  **Start the Backend**
    ```bash
    ./start_server.sh
    ```
    The application will run on `http://localhost:5000`.

## ğŸ“‚ Project Structure

```
ipo_qa/
â”œâ”€â”€ src/                # Application Source Code
â”‚   â”œâ”€â”€ app.py          # Main Flask Application
â”‚   â”œâ”€â”€ utils/          # RAG Engines & Helpers
â”‚   â”œâ”€â”€ static/         # Frontend Assets
â”‚   â””â”€â”€ templates/      # HTML Templates
â”œâ”€â”€ scripts/            # Utility Scripts
â”‚   â”œâ”€â”€ build_kg.py     # KG Extraction Pipeline
â”‚   â””â”€â”€ visualize_kg.py # Graph Visualization
â”œâ”€â”€ evaluation/         # Benchmarking Tools
â”‚   â”œâ”€â”€ evaluate_*.py   # Evaluation Scripts
â”‚   â””â”€â”€ data/           # Test Datasets
â”œâ”€â”€ docs/               # Documentation
â”‚   â”œâ”€â”€ architecture.md # Detailed Design
â”‚   â””â”€â”€ case_study.md   # Project Case Study
â”œâ”€â”€ data/               # Processed Data (GitInored)
â””â”€â”€ requirements.txt    # Dependencies
```

## ğŸ§ª Evaluation

To run the benchmark suite:
```bash
python evaluation/evaluate_complex.py
```
This runs 10 complex reasoning questions against KG, Vector, and Hybrid modes to compare performance.

## ğŸ“ License

MIT License. See [LICENSE](LICENSE) for details.
